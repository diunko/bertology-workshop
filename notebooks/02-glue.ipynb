{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dvl/nlp/learning/bertology-workshop\n"
     ]
    },
    {
     "data": {
      "text/plain": "'/dvl/nlp/learning/bertology-workshop'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bootstrap():\n",
    "    dir = %pwd\n",
    "    if dir.endswith('/bertology-workshop'):\n",
    "        # we're set\n",
    "        pass\n",
    "    elif dir.endswith('bertology-workshop/notebooks'):\n",
    "        # looks like we're inside jupyter\n",
    "        %cd ..\n",
    "    elif dir == '/content':\n",
    "        # looks like we're on colab\n",
    "        !git clone git://github.com:diunko/bertology-workshop\n",
    "        %cd bertology-workshop\n",
    "    else:\n",
    "        raise RuntimeError('I have to be launched from colab or from '\n",
    "                           'bertology-workshop/notebooks')\n",
    "    dir = %pwd\n",
    "    return dir\n",
    "bootstrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33m      WARNING: Cannot remove entries from nonexistent file /dvl/nlp/learning/bertology-workshop/.pyenv/lib/python3.6/site-packages/easy-install.pth\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!make requirements > /dev/null"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download GLUE data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Nothing to be done for `data/glue'.\r\n"
     ]
    }
   ],
   "source": [
    "!make data/glue"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.file_utils:PyTorch version 1.5.0 available.\n",
      "INFO:transformers.file_utils:TensorFlow version 2.2.0 available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "\n",
    "from bertology.glue import GLUETransformer\n",
    "from bertology.glue import generic_train\n",
    "from bertology.utils import args_to_dict, dict_to_args\n",
    "\n",
    "import argparse\n",
    "from argparse import Namespace"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--output_dir OUTPUT_DIR] [--fp16]\n",
      "                             [--fp16_opt_level FP16_OPT_LEVEL] [--n_gpu N_GPU]\n",
      "                             [--n_tpu_cores N_TPU_CORES]\n",
      "                             [--max_grad_norm MAX_GRAD_NORM] [--fast_dev_run]\n",
      "                             [--overfit_pct OVERFIT_PCT] [--do_train]\n",
      "                             [--do_predict]\n",
      "                             [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                             [--seed SEED] --model_name_or_path\n",
      "                             MODEL_NAME_OR_PATH [--config_name CONFIG_NAME]\n",
      "                             [--tokenizer_name TOKENIZER_NAME]\n",
      "                             [--cache_dir CACHE_DIR]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--adam_epsilon ADAM_EPSILON]\n",
      "                             [--warmup_steps WARMUP_STEPS]\n",
      "                             [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                             [--train_batch_size TRAIN_BATCH_SIZE]\n",
      "                             [--eval_batch_size EVAL_BATCH_SIZE]\n",
      "                             [--max_seq_length MAX_SEQ_LENGTH] --task TASK\n",
      "                             --data_dir DATA_DIR [--overwrite_cache]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --output_dir OUTPUT_DIR\n",
      "                        The output directory where the model predictions and\n",
      "                        checkpoints will be written.\n",
      "  --fp16                Whether to use 16-bit (mixed) precision (through\n",
      "                        NVIDIA apex) instead of 32-bit\n",
      "  --fp16_opt_level FP16_OPT_LEVEL\n",
      "                        For fp16: Apex AMP optimization level selected in\n",
      "                        ['O0', 'O1', 'O2', and 'O3'].See details at\n",
      "                        https://nvidia.github.io/apex/amp.html\n",
      "  --n_gpu N_GPU\n",
      "  --n_tpu_cores N_TPU_CORES\n",
      "  --max_grad_norm MAX_GRAD_NORM\n",
      "                        Max gradient norm.\n",
      "  --fast_dev_run        Debugging: everything on one batch\n",
      "  --overfit_pct OVERFIT_PCT\n",
      "                        Debugging: run training on fraction of train\n",
      "  --do_train            Whether to run training.\n",
      "  --do_predict          Whether to run predictions on the test set.\n",
      "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n",
      "                        Number of updates steps to accumulate before\n",
      "                        performing a backward/update pass.\n",
      "  --seed SEED           random seed for initialization\n",
      "  --model_name_or_path MODEL_NAME_OR_PATH\n",
      "                        Path to pretrained model or model identifier from\n",
      "                        huggingface.co/models\n",
      "  --config_name CONFIG_NAME\n",
      "                        Pretrained config name or path if not the same as\n",
      "                        model_name\n",
      "  --tokenizer_name TOKENIZER_NAME\n",
      "                        Pretrained tokenizer name or path if not the same as\n",
      "                        model_name\n",
      "  --cache_dir CACHE_DIR\n",
      "                        Where do you want to store the pre-trained models\n",
      "                        downloaded from s3\n",
      "  --learning_rate LEARNING_RATE\n",
      "                        The initial learning rate for Adam.\n",
      "  --weight_decay WEIGHT_DECAY\n",
      "                        Weight decay if we apply some.\n",
      "  --adam_epsilon ADAM_EPSILON\n",
      "                        Epsilon for Adam optimizer.\n",
      "  --warmup_steps WARMUP_STEPS\n",
      "                        Linear warmup over warmup_steps.\n",
      "  --num_train_epochs NUM_TRAIN_EPOCHS\n",
      "                        Total number of training epochs to perform.\n",
      "  --train_batch_size TRAIN_BATCH_SIZE\n",
      "  --eval_batch_size EVAL_BATCH_SIZE\n",
      "  --max_seq_length MAX_SEQ_LENGTH\n",
      "                        The maximum total input sequence length after\n",
      "                        tokenization. Sequences longer than this will be\n",
      "                        truncated, sequences shorter will be padded.\n",
      "  --task TASK           The GLUE task to run\n",
      "  --data_dir DATA_DIR   The input data dir. Should contain the training files\n",
      "                        for the CoNLL-2003 NER task.\n",
      "  --overwrite_cache     Overwrite the cached training and evaluation sets\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[0;31mSystemExit\u001B[0m\u001B[0;31m:\u001B[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dvl/nlp/learning/bertology-workshop/.pyenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "GLUETransformer.from_dict(dict(help=True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bertology.glue:hparams: {\n",
      "  \"output_dir\": null,\n",
      "  \"fp16\": false,\n",
      "  \"fp16_opt_level\": \"O1\",\n",
      "  \"n_gpu\": -1,\n",
      "  \"n_tpu_cores\": 0,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"fast_dev_run\": true,\n",
      "  \"overfit_pct\": 0.0,\n",
      "  \"do_train\": false,\n",
      "  \"do_predict\": false,\n",
      "  \"gradient_accumulation_steps\": 1,\n",
      "  \"seed\": 42,\n",
      "  \"model_name_or_path\": \"bert-base-cased\",\n",
      "  \"config_name\": \"\",\n",
      "  \"tokenizer_name\": \"\",\n",
      "  \"cache_dir\": \"\",\n",
      "  \"learning_rate\": 5e-05,\n",
      "  \"weight_decay\": 0.0,\n",
      "  \"adam_epsilon\": 1e-08,\n",
      "  \"warmup_steps\": 0,\n",
      "  \"num_train_epochs\": 3,\n",
      "  \"train_batch_size\": 1,\n",
      "  \"eval_batch_size\": 32,\n",
      "  \"max_seq_length\": 128,\n",
      "  \"task\": \"mrpc\",\n",
      "  \"data_dir\": \"data/glue/MRPC\",\n",
      "  \"overwrite_cache\": false\n",
      "}\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /Users/diunko/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /Users/diunko/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /Users/diunko/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-cased-pytorch_model.bin from cache at /Users/diunko/.cache/torch/transformers/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
      "INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'output_dir': None,\n 'fp16': False,\n 'fp16_opt_level': 'O1',\n 'n_gpu': -1,\n 'n_tpu_cores': 0,\n 'max_grad_norm': 1.0,\n 'fast_dev_run': True,\n 'overfit_pct': 0.0,\n 'do_train': False,\n 'do_predict': False,\n 'gradient_accumulation_steps': 1,\n 'seed': 42,\n 'model_name_or_path': 'bert-base-cased',\n 'config_name': '',\n 'tokenizer_name': '',\n 'cache_dir': '',\n 'learning_rate': 5e-05,\n 'weight_decay': 0.0,\n 'adam_epsilon': 1e-08,\n 'warmup_steps': 0,\n 'num_train_epochs': 3,\n 'train_batch_size': 1,\n 'eval_batch_size': 32,\n 'max_seq_length': 128,\n 'task': 'mrpc',\n 'data_dir': 'data/glue/MRPC',\n 'overwrite_cache': False,\n 'glue_output_mode': 'classification'}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams = dict(\n",
    "    model_name_or_path='bert-base-cased',\n",
    "    data_dir='data/glue/MRPC',\n",
    "    n_gpu=-1,\n",
    "    train_batch_size=1,\n",
    "    task='mrpc',\n",
    "    fast_dev_run=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# hyperparameter_defaults = dict(\n",
    "#     dropout = 0.5,\n",
    "#     channels_one = 16,\n",
    "#     channels_two = 32,\n",
    "#     batch_size = 100,\n",
    "#     learning_rate = 0.001,\n",
    "#     epochs = 2,\n",
    "#     )\n",
    "# \n",
    "# wandb.init(config=hyperparameter_defaults, project=\"pytorch-cnn-fashion\")\n",
    "# config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = GLUETransformer.from_dict(hparams)\n",
    "\n",
    "args = model.hparams\n",
    "args_to_dict(model.hparams)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning:Running in fast_dev_run mode: will run a full train, val and test loop using a single batch\n"
     ]
    }
   ],
   "source": [
    "if args.output_dir is None:\n",
    "    args.output_dir = os.path.join(\"./runs\", f\"{args.task}_{time.strftime('%Y%m%d_%H%M%S')}\",)\n",
    "    os.makedirs(args.output_dir)\n",
    "\n",
    "trainer = generic_train(model, args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bertology.glue:Creating features from dataset file at data/glue/MRPC\n",
      "INFO:transformers.data.processors.glue:LOOKING AT data/glue/MRPC/train.tsv\n",
      "INFO:transformers.data.processors.glue:*** Example ***\n",
      "INFO:transformers.data.processors.glue:guid: train-1\n",
      "INFO:transformers.data.processors.glue:features: InputFeatures(input_ids=[101, 7277, 2180, 5303, 4806, 1117, 1711, 117, 2292, 1119, 1270, 107, 1103, 7737, 107, 117, 1104, 9938, 4267, 12223, 21811, 1117, 2554, 119, 102, 11336, 6732, 3384, 1106, 1140, 1112, 1178, 107, 1103, 7737, 107, 117, 7277, 2180, 5303, 4806, 1117, 1711, 1104, 9938, 4267, 12223, 21811, 1117, 2554, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "INFO:transformers.data.processors.glue:*** Example ***\n",
      "INFO:transformers.data.processors.glue:guid: train-2\n",
      "INFO:transformers.data.processors.glue:features: InputFeatures(input_ids=[101, 10684, 2599, 9717, 1161, 2205, 11288, 1377, 112, 188, 1196, 4147, 1103, 4129, 1106, 19770, 2787, 1107, 1772, 1111, 109, 123, 119, 126, 3775, 119, 102, 10684, 2599, 9717, 1161, 3306, 11288, 1377, 112, 188, 1107, 1876, 1111, 109, 5691, 1495, 1550, 1105, 1962, 1122, 1106, 19770, 2787, 1111, 109, 122, 119, 129, 3775, 1107, 1772, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "INFO:transformers.data.processors.glue:*** Example ***\n",
      "INFO:transformers.data.processors.glue:guid: train-3\n",
      "INFO:transformers.data.processors.glue:features: InputFeatures(input_ids=[101, 1220, 1125, 1502, 1126, 16355, 1113, 1103, 4639, 1113, 1340, 1275, 117, 4733, 1103, 6527, 1111, 4688, 117, 1119, 1896, 119, 102, 1212, 1340, 1275, 117, 1103, 2062, 112, 188, 5032, 1125, 1502, 1126, 16355, 1113, 1103, 4639, 117, 4733, 1103, 16454, 1111, 4688, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "INFO:transformers.data.processors.glue:*** Example ***\n",
      "INFO:transformers.data.processors.glue:guid: train-4\n",
      "INFO:transformers.data.processors.glue:features: InputFeatures(input_ids=[101, 5596, 5347, 19297, 14748, 1942, 117, 22515, 1830, 6117, 1127, 1146, 1627, 18748, 117, 1137, 125, 119, 125, 110, 117, 1120, 138, 109, 125, 119, 4376, 117, 1515, 2206, 1383, 170, 1647, 1344, 1104, 138, 109, 125, 119, 4667, 119, 102, 22515, 1830, 6117, 4874, 1406, 18748, 117, 1137, 125, 119, 127, 110, 117, 1106, 1383, 170, 1647, 5134, 1344, 1120, 138, 109, 125, 119, 4667, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "INFO:transformers.data.processors.glue:*** Example ***\n",
      "INFO:transformers.data.processors.glue:guid: train-5\n",
      "INFO:transformers.data.processors.glue:features: InputFeatures(input_ids=[101, 1109, 4482, 3152, 109, 123, 119, 1429, 117, 1137, 1164, 1429, 3029, 117, 1106, 1601, 5286, 1120, 109, 1626, 119, 4062, 1113, 1103, 1203, 1365, 9924, 7855, 119, 102, 153, 2349, 111, 142, 13619, 119, 6117, 4874, 109, 122, 119, 5519, 1137, 129, 3029, 1106, 109, 1626, 119, 5347, 1113, 1103, 1203, 1365, 9924, 7855, 1113, 5286, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "INFO:bertology.glue:Saving features into cached file data/glue/MRPC/cached_train_bert-base-cased_128\n",
      "INFO:bertology.glue:Creating features from dataset file at data/glue/MRPC\n",
      "INFO:transformers.data.processors.glue:*** Example ***\n",
      "INFO:transformers.data.processors.glue:guid: dev-1\n",
      "INFO:transformers.data.processors.glue:features: InputFeatures(input_ids=[101, 1124, 1163, 1103, 11785, 1200, 14301, 16288, 1671, 2144, 112, 189, 4218, 1103, 1419, 112, 188, 1263, 118, 1858, 3213, 5564, 119, 102, 107, 1109, 11785, 1200, 14301, 16288, 1671, 1674, 1136, 4218, 1412, 1263, 118, 1858, 3213, 5564, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "INFO:transformers.data.processors.glue:*** Example ***\n",
      "INFO:transformers.data.processors.glue:guid: dev-2\n",
      "INFO:transformers.data.processors.glue:features: InputFeatures(input_ids=[101, 7085, 12149, 25294, 1163, 16890, 6617, 18982, 5687, 1103, 8612, 6716, 1105, 1350, 1977, 1106, 1606, 1117, 1263, 1201, 1104, 2013, 1107, 1103, 1594, 119, 102, 1230, 1676, 1163, 1119, 1108, 107, 1620, 3029, 1481, 1667, 6096, 107, 1105, 1350, 1977, 1106, 1606, 1117, 1201, 1104, 2013, 1107, 1103, 1594, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "INFO:transformers.data.processors.glue:*** Example ***\n",
      "INFO:transformers.data.processors.glue:guid: dev-3\n",
      "INFO:transformers.data.processors.glue:features: InputFeatures(input_ids=[101, 1109, 8876, 1108, 1120, 13096, 119, 5556, 6798, 1179, 1222, 1103, 6798, 1179, 117, 3596, 1113, 1103, 4912, 117, 1105, 1120, 122, 119, 1743, 1580, 1475, 1222, 1103, 4614, 175, 4047, 1665, 117, 1145, 3596, 119, 102, 1109, 8876, 1108, 1120, 13096, 119, 5603, 6798, 1179, 18283, 3663, 134, 117, 9024, 3596, 1113, 1103, 4912, 117, 1105, 1120, 122, 119, 25724, 1475, 1222, 1103, 4614, 175, 4047, 1665, 24890, 2271, 134, 117, 1205, 121, 119, 122, 3029, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "INFO:transformers.data.processors.glue:*** Example ***\n",
      "INFO:transformers.data.processors.glue:guid: dev-4\n",
      "INFO:transformers.data.processors.glue:features: InputFeatures(input_ids=[101, 1109, 9835, 118, 140, 19368, 1110, 2613, 1235, 1357, 1106, 4958, 1191, 1122, 1209, 1322, 18649, 170, 3234, 119, 102, 1109, 9835, 118, 140, 19368, 1717, 9031, 1115, 1122, 1209, 4958, 1107, 1357, 2480, 1106, 1322, 18649, 170, 3234, 1196, 1103, 185, 19123, 3377, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "INFO:transformers.data.processors.glue:*** Example ***\n",
      "INFO:transformers.data.processors.glue:guid: dev-5\n",
      "INFO:transformers.data.processors.glue:features: InputFeatures(input_ids=[101, 1302, 4595, 1138, 1151, 1383, 1111, 1103, 2987, 1137, 1103, 4771, 3443, 119, 102, 1302, 4595, 1138, 1151, 1383, 1111, 1103, 4771, 1137, 2987, 2740, 117, 1133, 20642, 1926, 1144, 12404, 1136, 5425, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "INFO:bertology.glue:Saving features into cached file data/glue/MRPC/cached_dev_bert-base-cased_128\n",
      "INFO:lightning:\n",
      "    | Name                                                   | Type                          | Params\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "0   | model                                                  | BertForSequenceClassification | 108 M \n",
      "1   | model.bert                                             | BertModel                     | 108 M \n",
      "2   | model.bert.embeddings                                  | BertEmbeddings                | 22 M  \n",
      "3   | model.bert.embeddings.word_embeddings                  | Embedding                     | 22 M  \n",
      "4   | model.bert.embeddings.position_embeddings              | Embedding                     | 393 K \n",
      "5   | model.bert.embeddings.token_type_embeddings            | Embedding                     | 1 K   \n",
      "6   | model.bert.embeddings.LayerNorm                        | LayerNorm                     | 1 K   \n",
      "7   | model.bert.embeddings.dropout                          | Dropout                       | 0     \n",
      "8   | model.bert.encoder                                     | BertEncoder                   | 85 M  \n",
      "9   | model.bert.encoder.layer                               | ModuleList                    | 85 M  \n",
      "10  | model.bert.encoder.layer.0                             | BertLayer                     | 7 M   \n",
      "11  | model.bert.encoder.layer.0.attention                   | BertAttention                 | 2 M   \n",
      "12  | model.bert.encoder.layer.0.attention.self              | BertSelfAttention             | 1 M   \n",
      "13  | model.bert.encoder.layer.0.attention.self.query        | Linear                        | 590 K \n",
      "14  | model.bert.encoder.layer.0.attention.self.key          | Linear                        | 590 K \n",
      "15  | model.bert.encoder.layer.0.attention.self.value        | Linear                        | 590 K \n",
      "16  | model.bert.encoder.layer.0.attention.self.dropout      | Dropout                       | 0     \n",
      "17  | model.bert.encoder.layer.0.attention.output            | BertSelfOutput                | 592 K \n",
      "18  | model.bert.encoder.layer.0.attention.output.dense      | Linear                        | 590 K \n",
      "19  | model.bert.encoder.layer.0.attention.output.LayerNorm  | LayerNorm                     | 1 K   \n",
      "20  | model.bert.encoder.layer.0.attention.output.dropout    | Dropout                       | 0     \n",
      "21  | model.bert.encoder.layer.0.intermediate                | BertIntermediate              | 2 M   \n",
      "22  | model.bert.encoder.layer.0.intermediate.dense          | Linear                        | 2 M   \n",
      "23  | model.bert.encoder.layer.0.output                      | BertOutput                    | 2 M   \n",
      "24  | model.bert.encoder.layer.0.output.dense                | Linear                        | 2 M   \n",
      "25  | model.bert.encoder.layer.0.output.LayerNorm            | LayerNorm                     | 1 K   \n",
      "26  | model.bert.encoder.layer.0.output.dropout              | Dropout                       | 0     \n",
      "27  | model.bert.encoder.layer.1                             | BertLayer                     | 7 M   \n",
      "28  | model.bert.encoder.layer.1.attention                   | BertAttention                 | 2 M   \n",
      "29  | model.bert.encoder.layer.1.attention.self              | BertSelfAttention             | 1 M   \n",
      "30  | model.bert.encoder.layer.1.attention.self.query        | Linear                        | 590 K \n",
      "31  | model.bert.encoder.layer.1.attention.self.key          | Linear                        | 590 K \n",
      "32  | model.bert.encoder.layer.1.attention.self.value        | Linear                        | 590 K \n",
      "33  | model.bert.encoder.layer.1.attention.self.dropout      | Dropout                       | 0     \n",
      "34  | model.bert.encoder.layer.1.attention.output            | BertSelfOutput                | 592 K \n",
      "35  | model.bert.encoder.layer.1.attention.output.dense      | Linear                        | 590 K \n",
      "36  | model.bert.encoder.layer.1.attention.output.LayerNorm  | LayerNorm                     | 1 K   \n",
      "37  | model.bert.encoder.layer.1.attention.output.dropout    | Dropout                       | 0     \n",
      "38  | model.bert.encoder.layer.1.intermediate                | BertIntermediate              | 2 M   \n",
      "39  | model.bert.encoder.layer.1.intermediate.dense          | Linear                        | 2 M   \n",
      "40  | model.bert.encoder.layer.1.output                      | BertOutput                    | 2 M   \n",
      "41  | model.bert.encoder.layer.1.output.dense                | Linear                        | 2 M   \n",
      "42  | model.bert.encoder.layer.1.output.LayerNorm            | LayerNorm                     | 1 K   \n",
      "43  | model.bert.encoder.layer.1.output.dropout              | Dropout                       | 0     \n",
      "44  | model.bert.encoder.layer.2                             | BertLayer                     | 7 M   \n",
      "45  | model.bert.encoder.layer.2.attention                   | BertAttention                 | 2 M   \n",
      "46  | model.bert.encoder.layer.2.attention.self              | BertSelfAttention             | 1 M   \n",
      "47  | model.bert.encoder.layer.2.attention.self.query        | Linear                        | 590 K \n",
      "48  | model.bert.encoder.layer.2.attention.self.key          | Linear                        | 590 K \n",
      "49  | model.bert.encoder.layer.2.attention.self.value        | Linear                        | 590 K \n",
      "50  | model.bert.encoder.layer.2.attention.self.dropout      | Dropout                       | 0     \n",
      "51  | model.bert.encoder.layer.2.attention.output            | BertSelfOutput                | 592 K \n",
      "52  | model.bert.encoder.layer.2.attention.output.dense      | Linear                        | 590 K \n",
      "53  | model.bert.encoder.layer.2.attention.output.LayerNorm  | LayerNorm                     | 1 K   \n",
      "54  | model.bert.encoder.layer.2.attention.output.dropout    | Dropout                       | 0     \n",
      "55  | model.bert.encoder.layer.2.intermediate                | BertIntermediate              | 2 M   \n",
      "56  | model.bert.encoder.layer.2.intermediate.dense          | Linear                        | 2 M   \n",
      "57  | model.bert.encoder.layer.2.output                      | BertOutput                    | 2 M   \n",
      "58  | model.bert.encoder.layer.2.output.dense                | Linear                        | 2 M   \n",
      "59  | model.bert.encoder.layer.2.output.LayerNorm            | LayerNorm                     | 1 K   \n",
      "60  | model.bert.encoder.layer.2.output.dropout              | Dropout                       | 0     \n",
      "61  | model.bert.encoder.layer.3                             | BertLayer                     | 7 M   \n",
      "62  | model.bert.encoder.layer.3.attention                   | BertAttention                 | 2 M   \n",
      "63  | model.bert.encoder.layer.3.attention.self              | BertSelfAttention             | 1 M   \n",
      "64  | model.bert.encoder.layer.3.attention.self.query        | Linear                        | 590 K \n",
      "65  | model.bert.encoder.layer.3.attention.self.key          | Linear                        | 590 K \n",
      "66  | model.bert.encoder.layer.3.attention.self.value        | Linear                        | 590 K \n",
      "67  | model.bert.encoder.layer.3.attention.self.dropout      | Dropout                       | 0     \n",
      "68  | model.bert.encoder.layer.3.attention.output            | BertSelfOutput                | 592 K \n",
      "69  | model.bert.encoder.layer.3.attention.output.dense      | Linear                        | 590 K \n",
      "70  | model.bert.encoder.layer.3.attention.output.LayerNorm  | LayerNorm                     | 1 K   \n",
      "71  | model.bert.encoder.layer.3.attention.output.dropout    | Dropout                       | 0     \n",
      "72  | model.bert.encoder.layer.3.intermediate                | BertIntermediate              | 2 M   \n",
      "73  | model.bert.encoder.layer.3.intermediate.dense          | Linear                        | 2 M   \n",
      "74  | model.bert.encoder.layer.3.output                      | BertOutput                    | 2 M   \n",
      "75  | model.bert.encoder.layer.3.output.dense                | Linear                        | 2 M   \n",
      "76  | model.bert.encoder.layer.3.output.LayerNorm            | LayerNorm                     | 1 K   \n",
      "77  | model.bert.encoder.layer.3.output.dropout              | Dropout                       | 0     \n",
      "78  | model.bert.encoder.layer.4                             | BertLayer                     | 7 M   \n",
      "79  | model.bert.encoder.layer.4.attention                   | BertAttention                 | 2 M   \n",
      "80  | model.bert.encoder.layer.4.attention.self              | BertSelfAttention             | 1 M   \n",
      "81  | model.bert.encoder.layer.4.attention.self.query        | Linear                        | 590 K \n",
      "82  | model.bert.encoder.layer.4.attention.self.key          | Linear                        | 590 K \n",
      "83  | model.bert.encoder.layer.4.attention.self.value        | Linear                        | 590 K \n",
      "84  | model.bert.encoder.layer.4.attention.self.dropout      | Dropout                       | 0     \n",
      "85  | model.bert.encoder.layer.4.attention.output            | BertSelfOutput                | 592 K \n",
      "86  | model.bert.encoder.layer.4.attention.output.dense      | Linear                        | 590 K \n",
      "87  | model.bert.encoder.layer.4.attention.output.LayerNorm  | LayerNorm                     | 1 K   \n",
      "88  | model.bert.encoder.layer.4.attention.output.dropout    | Dropout                       | 0     \n",
      "89  | model.bert.encoder.layer.4.intermediate                | BertIntermediate              | 2 M   \n",
      "90  | model.bert.encoder.layer.4.intermediate.dense          | Linear                        | 2 M   \n",
      "91  | model.bert.encoder.layer.4.output                      | BertOutput                    | 2 M   \n",
      "92  | model.bert.encoder.layer.4.output.dense                | Linear                        | 2 M   \n",
      "93  | model.bert.encoder.layer.4.output.LayerNorm            | LayerNorm                     | 1 K   \n",
      "94  | model.bert.encoder.layer.4.output.dropout              | Dropout                       | 0     \n",
      "95  | model.bert.encoder.layer.5                             | BertLayer                     | 7 M   \n",
      "96  | model.bert.encoder.layer.5.attention                   | BertAttention                 | 2 M   \n",
      "97  | model.bert.encoder.layer.5.attention.self              | BertSelfAttention             | 1 M   \n",
      "98  | model.bert.encoder.layer.5.attention.self.query        | Linear                        | 590 K \n",
      "99  | model.bert.encoder.layer.5.attention.self.key          | Linear                        | 590 K \n",
      "100 | model.bert.encoder.layer.5.attention.self.value        | Linear                        | 590 K \n",
      "101 | model.bert.encoder.layer.5.attention.self.dropout      | Dropout                       | 0     \n",
      "102 | model.bert.encoder.layer.5.attention.output            | BertSelfOutput                | 592 K \n",
      "103 | model.bert.encoder.layer.5.attention.output.dense      | Linear                        | 590 K \n",
      "104 | model.bert.encoder.layer.5.attention.output.LayerNorm  | LayerNorm                     | 1 K   \n",
      "105 | model.bert.encoder.layer.5.attention.output.dropout    | Dropout                       | 0     \n",
      "106 | model.bert.encoder.layer.5.intermediate                | BertIntermediate              | 2 M   \n",
      "107 | model.bert.encoder.layer.5.intermediate.dense          | Linear                        | 2 M   \n",
      "108 | model.bert.encoder.layer.5.output                      | BertOutput                    | 2 M   \n",
      "109 | model.bert.encoder.layer.5.output.dense                | Linear                        | 2 M   \n",
      "110 | model.bert.encoder.layer.5.output.LayerNorm            | LayerNorm                     | 1 K   \n",
      "111 | model.bert.encoder.layer.5.output.dropout              | Dropout                       | 0     \n",
      "112 | model.bert.encoder.layer.6                             | BertLayer                     | 7 M   \n",
      "113 | model.bert.encoder.layer.6.attention                   | BertAttention                 | 2 M   \n",
      "114 | model.bert.encoder.layer.6.attention.self              | BertSelfAttention             | 1 M   \n",
      "115 | model.bert.encoder.layer.6.attention.self.query        | Linear                        | 590 K \n",
      "116 | model.bert.encoder.layer.6.attention.self.key          | Linear                        | 590 K \n",
      "117 | model.bert.encoder.layer.6.attention.self.value        | Linear                        | 590 K \n",
      "118 | model.bert.encoder.layer.6.attention.self.dropout      | Dropout                       | 0     \n",
      "119 | model.bert.encoder.layer.6.attention.output            | BertSelfOutput                | 592 K \n",
      "120 | model.bert.encoder.layer.6.attention.output.dense      | Linear                        | 590 K \n",
      "121 | model.bert.encoder.layer.6.attention.output.LayerNorm  | LayerNorm                     | 1 K   \n",
      "122 | model.bert.encoder.layer.6.attention.output.dropout    | Dropout                       | 0     \n",
      "123 | model.bert.encoder.layer.6.intermediate                | BertIntermediate              | 2 M   \n",
      "124 | model.bert.encoder.layer.6.intermediate.dense          | Linear                        | 2 M   \n",
      "125 | model.bert.encoder.layer.6.output                      | BertOutput                    | 2 M   \n",
      "126 | model.bert.encoder.layer.6.output.dense                | Linear                        | 2 M   \n",
      "127 | model.bert.encoder.layer.6.output.LayerNorm            | LayerNorm                     | 1 K   \n",
      "128 | model.bert.encoder.layer.6.output.dropout              | Dropout                       | 0     \n",
      "129 | model.bert.encoder.layer.7                             | BertLayer                     | 7 M   \n",
      "130 | model.bert.encoder.layer.7.attention                   | BertAttention                 | 2 M   \n",
      "131 | model.bert.encoder.layer.7.attention.self              | BertSelfAttention             | 1 M   \n",
      "132 | model.bert.encoder.layer.7.attention.self.query        | Linear                        | 590 K \n",
      "133 | model.bert.encoder.layer.7.attention.self.key          | Linear                        | 590 K \n",
      "134 | model.bert.encoder.layer.7.attention.self.value        | Linear                        | 590 K \n",
      "135 | model.bert.encoder.layer.7.attention.self.dropout      | Dropout                       | 0     \n",
      "136 | model.bert.encoder.layer.7.attention.output            | BertSelfOutput                | 592 K \n",
      "137 | model.bert.encoder.layer.7.attention.output.dense      | Linear                        | 590 K \n",
      "138 | model.bert.encoder.layer.7.attention.output.LayerNorm  | LayerNorm                     | 1 K   \n",
      "139 | model.bert.encoder.layer.7.attention.output.dropout    | Dropout                       | 0     \n",
      "140 | model.bert.encoder.layer.7.intermediate                | BertIntermediate              | 2 M   \n",
      "141 | model.bert.encoder.layer.7.intermediate.dense          | Linear                        | 2 M   \n",
      "142 | model.bert.encoder.layer.7.output                      | BertOutput                    | 2 M   \n",
      "143 | model.bert.encoder.layer.7.output.dense                | Linear                        | 2 M   \n",
      "144 | model.bert.encoder.layer.7.output.LayerNorm            | LayerNorm                     | 1 K   \n",
      "145 | model.bert.encoder.layer.7.output.dropout              | Dropout                       | 0     \n",
      "146 | model.bert.encoder.layer.8                             | BertLayer                     | 7 M   \n",
      "147 | model.bert.encoder.layer.8.attention                   | BertAttention                 | 2 M   \n",
      "148 | model.bert.encoder.layer.8.attention.self              | BertSelfAttention             | 1 M   \n",
      "149 | model.bert.encoder.layer.8.attention.self.query        | Linear                        | 590 K \n",
      "150 | model.bert.encoder.layer.8.attention.self.key          | Linear                        | 590 K \n",
      "151 | model.bert.encoder.layer.8.attention.self.value        | Linear                        | 590 K \n",
      "152 | model.bert.encoder.layer.8.attention.self.dropout      | Dropout                       | 0     \n",
      "153 | model.bert.encoder.layer.8.attention.output            | BertSelfOutput                | 592 K \n",
      "154 | model.bert.encoder.layer.8.attention.output.dense      | Linear                        | 590 K \n",
      "155 | model.bert.encoder.layer.8.attention.output.LayerNorm  | LayerNorm                     | 1 K   \n",
      "156 | model.bert.encoder.layer.8.attention.output.dropout    | Dropout                       | 0     \n",
      "157 | model.bert.encoder.layer.8.intermediate                | BertIntermediate              | 2 M   \n",
      "158 | model.bert.encoder.layer.8.intermediate.dense          | Linear                        | 2 M   \n",
      "159 | model.bert.encoder.layer.8.output                      | BertOutput                    | 2 M   \n",
      "160 | model.bert.encoder.layer.8.output.dense                | Linear                        | 2 M   \n",
      "161 | model.bert.encoder.layer.8.output.LayerNorm            | LayerNorm                     | 1 K   \n",
      "162 | model.bert.encoder.layer.8.output.dropout              | Dropout                       | 0     \n",
      "163 | model.bert.encoder.layer.9                             | BertLayer                     | 7 M   \n",
      "164 | model.bert.encoder.layer.9.attention                   | BertAttention                 | 2 M   \n",
      "165 | model.bert.encoder.layer.9.attention.self              | BertSelfAttention             | 1 M   \n",
      "166 | model.bert.encoder.layer.9.attention.self.query        | Linear                        | 590 K \n",
      "167 | model.bert.encoder.layer.9.attention.self.key          | Linear                        | 590 K \n",
      "168 | model.bert.encoder.layer.9.attention.self.value        | Linear                        | 590 K \n",
      "169 | model.bert.encoder.layer.9.attention.self.dropout      | Dropout                       | 0     \n",
      "170 | model.bert.encoder.layer.9.attention.output            | BertSelfOutput                | 592 K \n",
      "171 | model.bert.encoder.layer.9.attention.output.dense      | Linear                        | 590 K \n",
      "172 | model.bert.encoder.layer.9.attention.output.LayerNorm  | LayerNorm                     | 1 K   \n",
      "173 | model.bert.encoder.layer.9.attention.output.dropout    | Dropout                       | 0     \n",
      "174 | model.bert.encoder.layer.9.intermediate                | BertIntermediate              | 2 M   \n",
      "175 | model.bert.encoder.layer.9.intermediate.dense          | Linear                        | 2 M   \n",
      "176 | model.bert.encoder.layer.9.output                      | BertOutput                    | 2 M   \n",
      "177 | model.bert.encoder.layer.9.output.dense                | Linear                        | 2 M   \n",
      "178 | model.bert.encoder.layer.9.output.LayerNorm            | LayerNorm                     | 1 K   \n",
      "179 | model.bert.encoder.layer.9.output.dropout              | Dropout                       | 0     \n",
      "180 | model.bert.encoder.layer.10                            | BertLayer                     | 7 M   \n",
      "181 | model.bert.encoder.layer.10.attention                  | BertAttention                 | 2 M   \n",
      "182 | model.bert.encoder.layer.10.attention.self             | BertSelfAttention             | 1 M   \n",
      "183 | model.bert.encoder.layer.10.attention.self.query       | Linear                        | 590 K \n",
      "184 | model.bert.encoder.layer.10.attention.self.key         | Linear                        | 590 K \n",
      "185 | model.bert.encoder.layer.10.attention.self.value       | Linear                        | 590 K \n",
      "186 | model.bert.encoder.layer.10.attention.self.dropout     | Dropout                       | 0     \n",
      "187 | model.bert.encoder.layer.10.attention.output           | BertSelfOutput                | 592 K \n",
      "188 | model.bert.encoder.layer.10.attention.output.dense     | Linear                        | 590 K \n",
      "189 | model.bert.encoder.layer.10.attention.output.LayerNorm | LayerNorm                     | 1 K   \n",
      "190 | model.bert.encoder.layer.10.attention.output.dropout   | Dropout                       | 0     \n",
      "191 | model.bert.encoder.layer.10.intermediate               | BertIntermediate              | 2 M   \n",
      "192 | model.bert.encoder.layer.10.intermediate.dense         | Linear                        | 2 M   \n",
      "193 | model.bert.encoder.layer.10.output                     | BertOutput                    | 2 M   \n",
      "194 | model.bert.encoder.layer.10.output.dense               | Linear                        | 2 M   \n",
      "195 | model.bert.encoder.layer.10.output.LayerNorm           | LayerNorm                     | 1 K   \n",
      "196 | model.bert.encoder.layer.10.output.dropout             | Dropout                       | 0     \n",
      "197 | model.bert.encoder.layer.11                            | BertLayer                     | 7 M   \n",
      "198 | model.bert.encoder.layer.11.attention                  | BertAttention                 | 2 M   \n",
      "199 | model.bert.encoder.layer.11.attention.self             | BertSelfAttention             | 1 M   \n",
      "200 | model.bert.encoder.layer.11.attention.self.query       | Linear                        | 590 K \n",
      "201 | model.bert.encoder.layer.11.attention.self.key         | Linear                        | 590 K \n",
      "202 | model.bert.encoder.layer.11.attention.self.value       | Linear                        | 590 K \n",
      "203 | model.bert.encoder.layer.11.attention.self.dropout     | Dropout                       | 0     \n",
      "204 | model.bert.encoder.layer.11.attention.output           | BertSelfOutput                | 592 K \n",
      "205 | model.bert.encoder.layer.11.attention.output.dense     | Linear                        | 590 K \n",
      "206 | model.bert.encoder.layer.11.attention.output.LayerNorm | LayerNorm                     | 1 K   \n",
      "207 | model.bert.encoder.layer.11.attention.output.dropout   | Dropout                       | 0     \n",
      "208 | model.bert.encoder.layer.11.intermediate               | BertIntermediate              | 2 M   \n",
      "209 | model.bert.encoder.layer.11.intermediate.dense         | Linear                        | 2 M   \n",
      "210 | model.bert.encoder.layer.11.output                     | BertOutput                    | 2 M   \n",
      "211 | model.bert.encoder.layer.11.output.dense               | Linear                        | 2 M   \n",
      "212 | model.bert.encoder.layer.11.output.LayerNorm           | LayerNorm                     | 1 K   \n",
      "213 | model.bert.encoder.layer.11.output.dropout             | Dropout                       | 0     \n",
      "214 | model.bert.pooler                                      | BertPooler                    | 590 K \n",
      "215 | model.bert.pooler.dense                                | Linear                        | 590 K \n",
      "216 | model.bert.pooler.activation                           | Tanh                          | 0     \n",
      "217 | model.dropout                                          | Dropout                       | 0     \n",
      "218 | model.classifier                                       | Linear                        | 1 K   \n",
      "/dvl/nlp/learning/bertology-workshop/vendor/pytorch-lightning/pytorch_lightning/utilities/warnings.py:18: RuntimeWarning: Displayed epoch numbers in the progress bar start from \"1\" until v0.6.x, but will start from \"0\" in v0.8.0.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "INFO:bertology.glue:Loading features from cached file data/glue/MRPC/cached_train_bert-base-cased_128\n",
      "/dvl/nlp/learning/bertology-workshop/vendor/pytorch-lightning/pytorch_lightning/utilities/warnings.py:18: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "INFO:bertology.glue:Loading features from cached file data/glue/MRPC/cached_dev_bert-base-cased_128\n",
      "/dvl/nlp/learning/bertology-workshop/vendor/pytorch-lightning/pytorch_lightning/utilities/warnings.py:18: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "INFO:bertology._base:***** Validation results *****\n",
      "INFO:bertology._base:acc = 0.25\n",
      "\n",
      "INFO:bertology._base:acc_and_f1 = 0.16346153846153846\n",
      "\n",
      "INFO:bertology._base:epoch = 0\n",
      "\n",
      "INFO:bertology._base:f1 = 0.07692307692307693\n",
      "\n",
      "INFO:bertology._base:loss = 0.62251216173172\n",
      "\n",
      "INFO:bertology._base:rate = 5e-05\n",
      "\n",
      "INFO:bertology._base:val_loss = 0.7720637321472168\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=1.0), HTML(value='')), …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e5548e71999e48839c2ae1203ed5077b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=1.0, style=Prog…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c7e4ce1b0ad247e99f97913acf0d2b74"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get metrics from the last checkpoint"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "checkpoints = list(sorted(glob.glob(\n",
    "    os.path.join(args.output_dir, \"checkpointepoch=*.ckpt\"), recursive=True)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /Users/diunko/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /Users/diunko/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /Users/diunko/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-cased-pytorch_model.bin from cache at /Users/diunko/.cache/torch/transformers/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
      "INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "INFO:bertology.glue:Loading features from cached file data/glue/MRPC/cached_train_bert-base-cased_128\n",
      "INFO:bertology.glue:Loading features from cached file data/glue/MRPC/cached_dev_bert-base-cased_128\n",
      "INFO:bertology.glue:Loading features from cached file data/glue/MRPC/cached_dev_bert-base-cased_128\n",
      "/dvl/nlp/learning/bertology-workshop/vendor/pytorch-lightning/pytorch_lightning/utilities/warnings.py:18: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "INFO:bertology._base:***** Test results *****\n",
      "INFO:bertology._base:acc = 0.34375\n",
      "\n",
      "INFO:bertology._base:acc_and_f1 = 0.171875\n",
      "\n",
      "INFO:bertology._base:avg_test_loss = 0.7505979537963867\n",
      "\n",
      "INFO:bertology._base:epoch = 0\n",
      "\n",
      "INFO:bertology._base:f1 = 0.0\n",
      "\n",
      "INFO:bertology._base:loss = 0.62251216173172\n",
      "\n",
      "INFO:bertology._base:rate = 5e-05\n",
      "\n",
      "INFO:bertology._base:val_loss = 0.7505979537963867\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Testing', layout=Layout(flex='2'), max=1.0, style=Progres…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d954e9e019df4a1c8f9af575a2f7c2ab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'acc': 0.34375,\n",
      " 'acc_and_f1': 0.171875,\n",
      " 'avg_test_loss': 0.7505979537963867,\n",
      " 'f1': 0.0,\n",
      " 'val_loss': 0.7505979537963867}\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = GLUETransformer.load_from_checkpoint(checkpoints[-1])\n",
    "trainer.test(model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}